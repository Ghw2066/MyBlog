---
title: "MapReduce Case--Tablejoin"
date: 2019-10-28T22:30:53+11:00
draft: false
categories:
 - Bigdata
tags:
 - hadoop
 - mapreduce
 - java
---

## Case Specification

There are two tables and both of them have the same foreign key. `join` in `hadoop`, is similar with the `join` in database. 

The first table, [students.txt]( https://github.com/Ghw2066/BigdataLearning/blob/master/lab/lab2/students.txt ), is a text file, each row shows a student's zid and his/her name, as shown below.

```
z1	Sara
z2	Ali
```

The second table, marks.txt, is a text file, each row shows a student's zid and two marks for his/her  assignments.

```
z1	90	80
z2	75	98
```

 Now code a Hadoop job to join (outer join) these two tables based on the zids. You output must look like: 

```
z1 Sara,90,80
z2 Ali,75,98
```

 To this happen, you need to create your own Writable class to store complex datatypes (based on existing Writable Datatypes such as IntWritable, Text, etc. ).The following link may help: [A magic link]( https://hadoop.apache.org/docs/r3.0.1/api/org/apache/hadoop/io/Writable.html)

## Solution

The code comes with comments for explanation.

```java

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;


public class Tablejoin {
    
    //This class is for store the result, which is "name Assign1 Assign2"
    public static class stdMarkWritable implements Writable {

        private Text name;
        private IntWritable Assign1;
        private IntWritable Assign2;
    //initialize the class, in order to detect the value to be joined
        public stdMarkWritable(){
            this.name = new Text("");
            this.Assign1 = new IntWritable(-1);
            this.Assign2 = new IntWritable(-1);
        }

        public stdMarkWritable(Text name, IntWritable assign1, IntWritable assign2) {
            this.name = name;
            Assign1 = assign1;
            Assign2 = assign2;
        }

        public Text getName() {
            return name;
        }

        public void setName(Text name) {
            this.name = name;
        }

        public IntWritable getAssign1() {
            return Assign1;
        }

        public void setAssign1(IntWritable assign1) {
            Assign1 = assign1;
        }

        public IntWritable getAssign2() {
            return Assign2;
        }

        public void setAssign2(IntWritable assign2) {
            Assign2 = assign2;
        }

        @Override
        public void write(DataOutput dataOutput) throws IOException {
            this.name.write(dataOutput);
            this.Assign1.write(dataOutput);
            this.Assign2.write(dataOutput);
        }

        @Override
        public void readFields(DataInput dataInput) throws IOException {
            this.name.readFields(dataInput);
            this.Assign1.readFields(dataInput);
            this.Assign2.readFields(dataInput);
        }

        @Override
        public String toString() {
            return  this.name.toString() + " " +
                    this.Assign1.toString() + " " +
                    this.Assign2.toString() ;
        }
    }
    //this class read the students.txt file and pass to reducer
    public static class StudentMapper extends Mapper<LongWritable, Text, Text, stdMarkWritable> {
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String parts[] = value.toString().split(",");
            stdMarkWritable val = new stdMarkWritable();
            val.setName(new Text(parts[1]));
            //zid as key, "name A1 A2" as value
            context.write(new Text(parts[0]), val);
        }
    }
    //read the marks.txt file and pass to reducer
    public static class MarkMapper extends Mapper<LongWritable, Text, Text, stdMarkWritable>{
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String parts[] = value.toString().split(",");
            stdMarkWritable val = new stdMarkWritable();
            val.setAssign1(new IntWritable(Integer.parseInt(parts[1])));
            val.setAssign2(new IntWritable(Integer.parseInt(parts[2])));
            //zid as key, "name A1 A2" as value
            context.write(new Text(parts[0]), val);
        }
    }
    //the only one reducer receives the key, value-list from two mapper
    public static class MyReducer extends Reducer<Text, stdMarkWritable, Text, stdMarkWritable>{
        @Override
        protected void reduce(Text key, Iterable<stdMarkWritable> values, Context context) throws IOException, InterruptedException {

            stdMarkWritable value = new stdMarkWritable();
            //join phase
            for(stdMarkWritable a: values)
                if(a.getName().toString().equals("")){
                    value.setAssign1(new IntWritable(a.getAssign1().get()));
                    value.setAssign2(new IntWritable(a.getAssign2().get()));
                }else {
                    value.setName(new Text(a.getName().toString()));
                }

            context.write(key, value);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Table join");

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(stdMarkWritable.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(stdMarkWritable.class);

        MultipleInputs.addInputPath(job, new Path(args[0]), TextInputFormat.class, StudentMapper.class);
        MultipleInputs.addInputPath(job, new Path(args[1]), TextInputFormat.class, MarkMapper.class);

        job.setReducerClass(MyReducer.class);
        FileOutputFormat.setOutputPath(job, new Path(args[2]));
        job.waitForCompletion(true);

    }
}

```









 